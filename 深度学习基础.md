# 向前传播与反向传播
向前传播：是指沿着计算图（或网络层）从输入到输出的信号传递过程。具体来说，就是将输入数据依次经过各层的线性变换（如全连接层的a=xw+b）和激活函数（如sigmoid,relu,softmax），最终在输出层得到模型的预测结果。这一步仅设计向前计算，不对参数做任何修改，是推理（inferencd）的核心流程。
反向传播：是指从输出层（通常是损失函数的结果）沿着计算图的反方向，利用链式法则将误差信号（梯度）逐层传递回去的过程，在每个节点，根据该节点的局部导数将上游传来的梯度乘以相应系数，再传给下游（前一层），从而累积得到对各参数（权重，偏置）的梯度，用于随后参数更新。 
# 手写数字识别
```py
import pickle
import numpy as np
from dataset.mnist import load_mnist
from common.functions import sigmoid, softmax

def get_data():
    (x_train, t_train), (x_test, t_test) = \
        load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

def init_network():
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y  = softmax(a3)
    return y
x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)           # 预测标签
    if p == t[i]:
        accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

```
## 为什么用测试集，而不是训练集？
训练集用于学习，即通过反向传播更新权重
测试集用于评估，即通过测试集评估模型性能，如准确率，召回率等。
## normalize是什么？
True时会把像素从0~255映射到0~1之间，即归一化。稳定训练和推理中的数值计算。
## flatten是什么？
将原本形状为 (1, 28, 28) 的三维数组“展平”成长度为 784 的一维向量，方便与全连接层的权重矩阵相乘；

## one_hot_label是什么？
True时，标签数据会变成仅包含0或1的向量，如标签为3时，标签向量会变成[0,0,0,1,0,0,0,0,0,0]。

## 为什么用pickle？
pickle模块实现了基本的数据序列和反序列化。通过pickle模块的序列化操作我们能够将程序中运行的对象信息保存到文件中去，永久存储；通过pickle模块的反序列化操作，我们能够从文件中创建上一次程序保存的对象。
## 为什么用sigmoid函数？
sigmoid函数是连续可导的，而ReLU函数在x=0处不可导，因此使用sigmoid函数作为激活函数。
## 为什么用softmax函数？
softmax函数将输出值变换成0~1之间的概率值，可以用于多分类问题。
## 为什么if p==t[i]时就说明预测准确？
p = np.argmax(y) 取出 y 中概率最高的下标，也就是模型认为最可能的类别；
t[i] 是该样本的真实标签（一个整数）；
当它们相等时，说明“模型预测的类别”正好和“真实标签”吻合，算一次正确分类。累计正确次数后除以样本总数，就是分类准确率。
# 批处理
```py
x, t = get_data()
network = init_network()

batch_size = 100  # 每个批次的图像数量
accuracy_cnt = 0

# 按 batch_size 为步长，生成批的起始索引
for i in range(0, len(x), batch_size):
    # —— 取出从 i 到 i+batch_size 的这一段图像，形状为 (batch_size, 784)
    x_batch = x[i:i+batch_size]
    # —— 将整个批一次性送入 predict，得到形状 (batch_size, 10) 的概率数组
    y_batch = predict(network, x_batch)
    # —— 在第 1 维（每行）上取最大值的索引，得到形状 (batch_size,) 的预测标签
    p = np.argmax(y_batch, axis=1)
    # —— 直接比较向量 p 与标签 t[i:i+batch_size]，True 计 1，累加正确数
    accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

```
## 为什么批处理能够加速？
减少 Python 层的循环开销
单张图片推理时，每一次都要从 Python 进入 NumPy 做一次矩阵运算，来回切换消耗较大；批处理只需一次函数调用，就能在底层一次性完成对整个批次的矩阵乘加运算。

充分利用向量化和底层优化
NumPy 底层调用了经过高度优化的 BLAS／LAPACK 库，能对大块连续内存做并行化 SIMD 运算和多线程并行，批量矩阵运算比多次小矩阵运算更能命中缓存、减少内存访问延迟。

降低调度与内存分配开销
批量操作时只分配一次输出数组，减少了多次分配、释放临时数组的成本。
# 如果只靠“数据驱动”，有没有可能学出一些奇怪但有效的“特征”?
模型可能学到“数据偏差”。举例：如果训练集里所有“5”后面都有模糊的阴影，而其他数字没有，网络就可能把“阴影”当作识别“5”的信号。
补充：这就是为什么要保证训练数据多样性、避免标签偏差，并在训练时加上正则化（如 Dropout）、数据增强（如随机旋转、裁剪）来减少这些“歧义特征”对模型的影响。