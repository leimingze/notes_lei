# 思想沿承
BERT是一个统一了多种思想的预训练模型，其沿承的核心思想包括：
transformer,预训练+微调范式
# 模型架构
BERT的模型架构是取了Transformer的Encoder部分堆叠而成
![alt text](assets/image48.png)
![alt text](assets/image49.png)
输入的文本序列首先会通过tokenizer（分词器）转换成input_ids，然后进入embedding层转换成特定维度的hidden_states,再经过encoder块。encoder块中堆叠起来N层encoder layer，BERT有两种规模的模型，分别是base版本，一种是large版本。通过Encoder编码之后的最顶层hidden_states最后经过prediction_heads就得到最终结果。
每一层的Encoder Layer都是Transformer中的Encoder Layer结构类似的层
![alt text](assets/image50.png)
已经通过 Embedding 层映射的 hidden_states 进入核心的 attention 机制，然后通过残差连接的机制和原输入相加，再经过一层 Intermediate 层得到最终输出。Intermediate 层是 BERT 的特殊称呼，其实就是一个线性层加上激活函数：
![alt text](assets/image51.png)
GELU 的核心思路为将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元
![alt text](assets/image52.png)
BERT 的 注意力机制和 Transformer 中 Encoder 的 自注意力机制几乎完全一致，但是 BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数
![alt text](assets/image53.png)
# 预训练任务MLM+NSP
预训练-微调范式的核心优势在于可以将预训练与微调分离，只要微调成本较低，即使预训练成本是之前的数倍，模型仍然拥有很大价值。
因此与训练数据的核心是需要极大的数据规模，但是人工标注不现实，需要无监督，但是LM使用的是上文预测下文，忽略了双向的语义关系，虽然transformer有位置编码，但是和直接拟合双向语义还是有本质区别。
## MLM
MLM思路很简单，再一个文本序列中随机遮蔽部分token,然后将所有未被遮蔽的token输入模型，要求模型根据输入预测被遮蔽的token。例如输入和输出可以是：
输入：I <MASK> you because you are <MASK>
输出：<MASK> - love; <MASK> - wonderful
MLM的固有缺陷：
1. 训练-推理不一致
   MLM在训练阶段引入mask，让模型学习填空
   但在下游任务中，真是文本并不会有mask，模型只能看到完整句子。
   也就是说，预训练和微调/推理（分类或其他任务）之间存在任务形式的不一致性
## NSP
即下一个句子预测，和心思限售股hi对于句级的NLU任务，例如问打匹配，自然语言推理等。
问答匹配是指，输入一个问题和若干问答，要求模型找出真正回答。
自然推理是指，输入一个前提和一个推理。判断推理是否是符合前提的。这样的任务都需要模型在局级去拟合关系。
NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文。例如，输入和输入可以是：
输入：
    Sentence A：I love you.
    Sentence B: Because you are wonderful.
输出：
    1（是连续上下文）

输入：
    Sentence A：I love you.
    Sentence B: Because today's dinner is so nice.
输出：
    0（不是连续上下文）
# 下游任务微调
作为自然语言处理（NLP）领域里程碑式的成果，BERT 的一个重大意义在于正式确立了**“预训练—微调”两阶段训练范式**该范式首先在海量无监督语料上进行预训练，从而获得通用的文本理解能力；随后再在具体的下游任务中，通过少量带标签的数据进行微调（fine-tuning），实现对任务需求的适应与优化。这种方法的核心优势在于：能否以较低成本的标注数据和计算资源，将预训练所获得的通用语言知识迁移到各种下游任务中。
为了提升迁移效率和兼容多种任务，BERT 设计了高度通用的输入格式和输出机制。具体而言，对于每一个输入文本序列，BERT 会在其开头添加一个特殊的 token：[CLS]。在后续 Transformer 编码过程中，[CLS] token 的最终隐状态被认为是整个序列的语义表示
# RoBERTa
之前说过，预训练微调的核心优势在于可以使用远大于之前训练数据的海量无监督预料进行预训练。因为在传统的深度学习范式中，对于每一个任务，我们需要从零训练一个模型，那么就无法使用太大的模型参数，否则需要极大规模的有监督数据才能让模型较好地拟合，成本太大。但在预训练-微调范式，我们在预训练阶段可以使用尽可能大量的训练数据，只需要一次预训练好的模型，后续在每一个下游任务上通过少量有监督数据微调即可。而 BERT 就使用了 13GB（3.3B token）的数据进行预训练，这相较于传统 NLP 来说是一个极其巨大的数据规模了。
## 优化一：去掉NSP预训练任务(*需要补充)
## 优化二:更大规模的与训练数据和预训练补偿
## 优化三：更大的bpe词表
# ALBERT
## 优化一：将Embedding参数进行分解
## 优化二：跨层进行参数共享
## 优化三：提出SOP预训练任务

# Q&A
## BERT可学习参数
隐藏层维度H：768
Encoder层数L:12
自注意力头数A:12
词汇表大小V:30522
前馈网络中间层维度I:3072
可学习参数主要分布在三个地方：嵌入层，编码器和池化层
### 嵌入层
负责将输入的词元，片段和位置信息转换为向量。包含三个部分和一层归一化
Token Embedding
一个巨大的查找表，矩阵大小为`词汇表大小`x`隐藏层维度`
V\*H=30522\*768=23,440,896(每一个词需要768个维度)
Position Embeddings位置嵌入
为了让模型理解词的顺序，Bert支持最大序列长度是512
MAX Sdquence Length\*H=512\*768=393216（每一个位置需要768个维度）
Segment Embeddings片段嵌入
用于区分两个句子：2\*H=2\*768=1536
Layer Normalization(层归一化)
三个嵌入向量相加后，会经过一个层归一化，又两个可学习参数gamma和beta，维度都是H
2*H=2\*768=1536
嵌入层总计: 23,440,896 + 393,216 + 1,536 + 1,536 = 23,837,184 (约 24M)
### 编码器
Bert-Base有12个完全相同的Encoder层堆叠而成
A 多头自注意力模块
1. Q,K,V线性变换
   模型需要将输入向量（维度H）分别通过三个线性层得到Q,K,V。
   权重矩阵W_qkv的维度事H\*(3\*H) （因为输入的x是L*H，现在需要XW+b，所以W应该是H\*H）
   偏置项
   3\*H
   总计=(H * 3 * H) + 3 * H = (768 * 3 * 768) + (3 * 768) = 1,769,472 + 2,304 = 1,771,776
2. 输出线性层
   多头注意力的结果被拼接起来，通过一个线性层变回维度H
（简单拼接只是机械合并），而通过W_0线性变化，Output=sigma(W_0_i*Head_i+b)
(H * H) + H = (768 * 768) + 768 = 589,824 + 768 = 590,592
3. 层归一化(Layer Normalization)
   2*H=2*768=1536
自注意力小计
1,771,776 + 590,592 + 1,536 = 2,363,904
B 前馈网络：
1. 第一个线性层（扩张）
   将维度从H扩张到I（3072）
   权重H\*I=768*3072=2359296
   偏置I=3072
2. 第二个线性层（收缩）
   将维度从I收缩到H
   权重H\*I=768*3072=2359296
   偏置H=768
3. 层归一化
   前馈网络的输出也有一个层归一化
   2*H=2*768=1536
【前馈网络模块小计】: 2,359,296 + 3,072 + 2,359,296 + 768 + 1,536 = 4,723,968

【单个Encoder层总计】: 2,363,904 (自注意力) + 4,723,968 (前馈网络) = 7,087,872

【12个Encoder层总计】: 7,087,872 * 12 = 85,054,464 (约 85M)
### 池化层
主要用于处理[CLS]标记的输出，以及进行下一句预测（NSP）等句子级别的任务，他的本质上是一个全连接层。
权重：H*H=768\*768=589,824
偏置项：H=768
总计：589,824 + 768 = 590,592


组件	可学习参数数量	约占 (百万)
嵌入层 (Embeddings)	23,837,184	23.8 M
编码器 (Encoders x 12)	85,054,464	85.1 M
池化层 (Pooler)	590,592	0.6 M
总计	109,482,240	~110 M