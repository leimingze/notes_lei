# 基于上下文的Embedding
专门用来做词向量的，通过预训练，
不只是训练一个Q矩阵，还可以把这个词的上下文信息融入到Q矩阵当中
# 为什么需要ELMo
传统词向量的局限：同一个词在不同上下文中，向量是一样的
ELMo解决了这个问题
# ELMo模型结构
```text
输入句子 → Token Embedding
          ↓
       BiLSTM Layer 1
          ↓
       BiLSTM Layer 2
          ↓
      三层特征输出（用于加权融合）
```
ELMo为每个词生成三层表示：
| 层次  | 内容              | 作用             |
| --- | --------------- | -------------- |
| 第0层 | Token embedding | 静态词向量（如 GloVe） |
| 第1层 | BiLSTM 第1层隐藏状态  | 捕捉局部语法、搭配等     |
| 第2层 | BiLSTM 第2层隐藏状态  | 捕捉更高层语义、句法结构等  |
# 输出词向量的方式
ELMo融合三层信息得到词向量：
```text
  W_1 * e + W_2 * l1_h + W_3 * l2_h
```
不同任务可根据需要侧重不同层（灵活！）